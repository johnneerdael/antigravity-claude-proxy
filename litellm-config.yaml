# LiteLLM configuration for Antigravity Claude Proxy
# For Docker: use http://antigravity-proxy:8080/v1
# For local: use http://localhost:8080/v1

model_list:
  # Claude models via Antigravity
  - model_name: claude-sonnet-4-5-thinking
    model_info:
      mode: chat
      supports_vision: true
      supports_function_calling: true
      supports_parallel_function_calling: true
      max_input_tokens: 200000
      max_output_tokens: 16000
    litellm_params:
      model: openai/claude-sonnet-4-5-thinking
      api_base: http://antigravity-proxy:8080/v1
      api_key: "not-needed"
      stream: true

  - model_name: claude-opus-4-5-thinking
    model_info:
      mode: chat
      supports_vision: true
      supports_function_calling: true
      supports_parallel_function_calling: true
      max_input_tokens: 200000
      max_output_tokens: 16000
    litellm_params:
      model: openai/claude-opus-4-5-thinking
      api_base: http://antigravity-proxy:8080/v1
      api_key: "not-needed"
      stream: true

  - model_name: claude-sonnet-4-5
    model_info:
      mode: chat
      supports_vision: true
      supports_function_calling: true
      supports_parallel_function_calling: true
      max_input_tokens: 200000
      max_output_tokens: 8192
    litellm_params:
      model: openai/claude-sonnet-4-5
      api_base: http://antigravity-proxy:8080/v1
      api_key: "not-needed"
      stream: true

  # Gemini models via Antigravity
  - model_name: gemini-3-flash
    model_info:
      mode: chat
      supports_vision: true
      supports_function_calling: true
      supports_parallel_function_calling: true
      max_input_tokens: 1048576
      max_output_tokens: 16384
    litellm_params:
      model: openai/gemini-3-flash
      api_base: http://antigravity-proxy:8080/v1
      api_key: "not-needed"
      stream: true

  - model_name: gemini-3-pro-high
    model_info:
      mode: chat
      supports_vision: true
      supports_function_calling: true
      supports_parallel_function_calling: true
      max_input_tokens: 1048576
      max_output_tokens: 16384
    litellm_params:
      model: openai/gemini-3-pro-high
      api_base: http://antigravity-proxy:8080/v1
      api_key: "not-needed"
      stream: true

  - model_name: gemini-3-pro-low
    model_info:
      mode: chat
      supports_vision: true
      supports_function_calling: true
      supports_parallel_function_calling: true
      max_input_tokens: 1048576
      max_output_tokens: 16384
    litellm_params:
      model: openai/gemini-3-pro-low
      api_base: http://antigravity-proxy:8080/v1
      api_key: "not-needed"
      stream: true
